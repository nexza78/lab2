{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №2 Код Хемминга и контрольные суммы"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Багильдинская Мария, БПМ-19-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вариант №4, длина слова с контрольными битами = 36, алгоритм контрольной суммы: CRC-16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к дана длина слова вместе с контольными битами, то определим сколько битов выделено для самого слова и контрольных бит:\n",
    "\n",
    "Учитывая, что контрольные биты находяся только на позициях с номерами, равными степеням двойки, то контрольные биты будут находится на позициях 0, 1, 3, 7, 15 и 31 (с учетом индексации с нуля). Итого 6 контрольных бит, значит само слово занимает 30 бит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [0, 1, 3, 7, 15, 31]\n",
    "lenw = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Текст для передачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модели обработки естественного языка (Natural language processing, NLP) на основе архитектуры Трансформеров, такие как BERT, RoBERTa, T5 или GPT3, успешно применяются в самых различных задачах и являются стандартом современных исследований в области NLP. Гибкость (универсальность) и надёжность Трансформеров способствовали их широкому распространению, что, в свою очередь, позволило легко адаптировать подобные модели для разнообразных задач обработки текстовых последовательностей, как в качестве seq2seq моделей для перевода, суммаризации, генерации текста и т.д., так и как самостоятельного энкодера для анализа тональности, частеречной разметки, машинного чтения и др. Главным изобретением Трансформеров стал механизм внутреннего внимания, который подсчитывает метрику схожести для всех возможных пар токенов входной последовательности независимо (параллельно), что позволяет избежать последовательной зависимости рекуррентных нейронных сетей. Благодаря этому механизму Трансформеры существенно превосходят более ранние модели обработки текстовых последовательностей, такие как LSTM.\n",
      "\n",
      "Однако модели на основе Трансформеров и их производные имеют свои ограничения. Так, механизм внутреннего внимания требует вычислений, квадратичных длине входной последовательности. С нынешними мощностями и размерами моделей это обычно ограничивает входную последовательность до 512 токенов и не позволяет напрямую применять Трансформеры к задачам, требующим большего контекста, таким как вопросно-ответные системы, суммаризация документов или классификация фрагментов генома. Естественным образом возникает два вопроса:\n",
      "\n",
      "1.\tМожно ли достичь тех же эмпирических результатов, что и традиционные «квадратичные» Трансформеры, но используя разреженные модели, у которых требования к вычислительной мощности и памяти возрастают линейно с увеличением входной последовательности?\n",
      "2.\tМожно ли теоретически обосновать то, что эти «линейные» Трансформеры сохраняют выразительность и гибкость своих «квадратичных» собратов?\n",
      "\n",
      "На оба эти вопроса авторы постарались ответить в своих недавних статьях. Так, в «ETC: Encoding Long and Structured Inputs in Transformers», представленной на конференции EMNLP 2020, было предложено построение расширенного Трансформера (Extended Transformer Construction, ETC) – новый метод разреженного внимания, в котором используется информация о структуре данных для ограничения числа подсчитываемых пар оценок сходства (similarity score). Это позволяет привести квадратичную зависимость к линейной и получить хорошие эмпирические результаты в NLP задачах.\n"
     ]
    }
   ],
   "source": [
    "text = '''Модели обработки естественного языка (Natural language processing, NLP) на основе архитектуры Трансформеров, такие как BERT, RoBERTa, T5 или GPT3, успешно применяются в самых различных задачах и являются стандартом современных исследований в области NLP. Гибкость (универсальность) и надёжность Трансформеров способствовали их широкому распространению, что, в свою очередь, позволило легко адаптировать подобные модели для разнообразных задач обработки текстовых последовательностей, как в качестве seq2seq моделей для перевода, суммаризации, генерации текста и т.д., так и как самостоятельного энкодера для анализа тональности, частеречной разметки, машинного чтения и др. Главным изобретением Трансформеров стал механизм внутреннего внимания, который подсчитывает метрику схожести для всех возможных пар токенов входной последовательности независимо (параллельно), что позволяет избежать последовательной зависимости рекуррентных нейронных сетей. Благодаря этому механизму Трансформеры существенно превосходят более ранние модели обработки текстовых последовательностей, такие как LSTM.\n",
    "\n",
    "Однако модели на основе Трансформеров и их производные имеют свои ограничения. Так, механизм внутреннего внимания требует вычислений, квадратичных длине входной последовательности. С нынешними мощностями и размерами моделей это обычно ограничивает входную последовательность до 512 токенов и не позволяет напрямую применять Трансформеры к задачам, требующим большего контекста, таким как вопросно-ответные системы, суммаризация документов или классификация фрагментов генома. Естественным образом возникает два вопроса:\n",
    "\n",
    "1.\tМожно ли достичь тех же эмпирических результатов, что и традиционные «квадратичные» Трансформеры, но используя разреженные модели, у которых требования к вычислительной мощности и памяти возрастают линейно с увеличением входной последовательности?\n",
    "2.\tМожно ли теоретически обосновать то, что эти «линейные» Трансформеры сохраняют выразительность и гибкость своих «квадратичных» собратов?\n",
    "\n",
    "На оба эти вопроса авторы постарались ответить в своих недавних статьях. Так, в «ETC: Encoding Long and Structured Inputs in Transformers», представленной на конференции EMNLP 2020, было предложено построение расширенного Трансформера (Extended Transformer Construction, ETC) – новый метод разреженного внимания, в котором используется информация о структуре данных для ограничения числа подсчитываемых пар оценок сходства (similarity score). Это позволяет привести квадратичную зависимость к линейной и получить хорошие эмпирические результаты в NLP задачах.'''\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего в тексте 90 уникальных символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(list(text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому закодируем 1 символ 7 битами. Сопоставление между символами и двоичным кодом представлено ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char</th>\n",
       "      <th>binary_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ф</td>\n",
       "      <td>0000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m</td>\n",
       "      <td>0000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>щ</td>\n",
       "      <td>0000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Г</td>\n",
       "      <td>0000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>»</td>\n",
       "      <td>0000101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  char binary_code\n",
       "0    ф     0000001\n",
       "1    m     0000010\n",
       "2    щ     0000011\n",
       "3    Г     0000100\n",
       "4    »     0000101"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_code = pd.DataFrame(data = set(list(text)), columns = ['char'])\n",
    "bin_code['binary_code'] = list('{0:07b}'.format(x) for x in range(1, 91))\n",
    "bin_code.head(5)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJQAAAAXCAYAAADtGGaiAAAD+0lEQVRoge2aTXISQRiGH2L2DDtXZnKCgBcIeABDPIASL5BkpSsTTiA5QeAEwRNIXLkyxJWuxJ070ZUrcfF21/ykkyLN/FnFW0Ux0zPMvP3199/AGmussUZV8cDzdyHQAz6mxgPgNdAFfphPFREAr4DL2FgIHJrjbTS/ScG8lkUPOAEeAl+BP+XS8UcIDJGg08IOzViIFOqqWGpL4wjxXAC12HjbjC3M9Vbx1JbCKdA0x0Mk56A8OtnglJsKNQIOzHETWVFVYZUnrVCn5dC5FybA2BwHaB6d8ugksZnhs56jiZ4Ac+Asw2cXia75Ht95V3kYALPU2KIMIi5sZPQcGx5eAH1k+ecZPTtPxBdijpTpuzmfFk9nKYyJuB0B15SX63VRirMy0iGvgxanbc5tSMnkZTnAFfIgmYvMqXbYbqI1KDp/6qH1v8IRbrPyUN9uGfdRqA5wvAIXX/RIKtAUf4PIO1wGaFG7sfP7ooYfzxoKudeui1kp1IwoVMThGzbKqFraqfc2uZmrLIMa0MiEkRsByqN6wC/E27ci9ZHzOaouXet9Q6E6SOuD2Hk7dj1ESfcusGWOrRUfmU8b5VLHKGwUiW3Efzt1HhdcD/EDeEPklYZoLm3Uj5qYMR/4JsmBeX9c5vskFWaI+P8E/gIX3B4hCoet8gJE9BKoE8XIz8AeEu4pUpBL4AOR0KzSjJFFd1GFV3RSe2DefQG8J1KQKRK49RpTw3Nkzi3/CRH/KcVXqSHyPO+QwR4izgPkFfpobn0HNx9PmivOSFrBAgm8ZY6LLKE7SGj3QYskxwlSlICoiVlUGK3hV3XFE2xbNByYz4Ls80pfnhZ9HEm59VB1os62vWGMLLuPrCQPuBpyTeCR45rtYLuwg8KtRYA86RxxH5BP+A2Jwmscddxz+4bbm4RI3pajNe4JCm11/MOvfV7aoBZ38LzCU15WoeLVjY3fY/PSvLrHNl9IY4sol0ljhntB4sJuIAWzIS1P79rBXQk2cPNf4OY/I2m0bZRuzMxvVjXoJvfnmVlva0Jy07Ro+IS8OPaRQMrai1s1lEAxOw25hLwN5Cku0ELUUEI4Tf2wMntFt6CH5gBRb8bOoUF1t1EsQsS/gwyhTnINRlS3SZzAJvAYLcI1mggoboNc5S5qD1QZNiwHwFO0GLYKHVD9fUVboY5QVQ1RK6CF1qNylZxBIjfbBD4hZdpB1vwElamhublL9TFGudcYeIYM4BwlzGeojVBlXKDe0h7wG3hJ9DehBlHfrAqw1ahtIR0Cb5Ex3Op4atzc5yoKvjlUmnNZ/FfJTdL885zDqjlULfUN+P9jM2/MqO6/PZfBHPhSNokl8L/wXGONNdbIAP8AwQTkro0YpJkAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию кодирования текста кодом Хемминга, одновременно считая контрольные суммы CRC-16. Для CRC-16 полиномом будет ![ ](attachment:image.png), то есть 11000000000000101. Контрольная сумма будет записана в последних 16 битах. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция divide - для вычисления CRC16 по прямому алгоритму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toList(x):\n",
    "  l = []\n",
    "  for i in range (0,len(x)):\n",
    "    l.append(int(x[i]))\n",
    "  return (l)\n",
    "\n",
    "def toString(x):\n",
    "  str1 =\"\"\n",
    "  for i in range (0,len(x)):\n",
    "    str1+=str(x[i])\n",
    "  return (str1)\n",
    "\n",
    "def divide(val1,val2 = '11000000000000101'):\n",
    "  a = [int(i) for i in list(str(int(val1)))]\n",
    "  b = [int(i) for i in val2]\n",
    "  while len(b) <= len(a):\n",
    "    del a[0]\n",
    "    for j in range(len(b)-1):\n",
    "      a[j] ^= b[j+1]\n",
    "    a = [int(i) for i in list(str(int(toString(a))))]\n",
    "  return toString(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0001000011110111001011011001001011000101100110111010101111001010100100110111101100101001010011011100000111011111001101011011101001011100110011010000011001011010100011101010000001010000110111100000010101001110101100101100011001100111001001111000011110110100000011111011100111001000000010011110000000100100101110010100110000001100110010110001101010010011000101101110110111011110011011100010101010101010101010000101011001100000001011100011011101010011100010010110001000011000101001010101011110011110001011111010001000110100000110110101010000110110111010010111001100111100100101001010011001010101110100111100010101111011001111001101101101110101010101101101110010011011110000010010111000101111010010011001011101011011010110111110101000010100010001000100111011100100010110000111101100000010101000001110101101111000010101111001101111110101010100110001110101011001101111000000111110101010010010100100111001100111101100100011101111000001100001101101000101101101011000000010011110110001101101100110110011110101110110011011010010001010011111110010011010010100011100100011110100001001111001001010000111111111001011110011101101011001101111010000100100101100001001010100110110101110100000110001100011111110101110100101000001110110011001101000011010001010010001000001010101111000110110110011000000000110110011110100000101101101101011100111001011100101110111000111000110000000010100000100111011011111010000100111000101101100001101110101100000110111100001100001001010000100110110001000011101001110010001111101010001010100011000110110100110110101001110001101010000010110010101100011000001101000111001101010100101110010001101110010001101110111011110011001100011011110110000111101010001110101000011110101010000110111100011100110110111101011000110001111011100101100100000110101111011010011001110111011110001111000110110001010101000100100010101001000111010011011000001101100010111111111100101010010011111001010010011010101000100010011000100000100001100011001111100000100111110100101011111101001001001001001111000001101001111101100111110110101000101010100000000011011110001101010111000001011100001111000111110001010110110100100110111100011101101111010011110001010011100010101000111010101100000101100110101000100100100010001001101011010100010011010101010101110011001111000101000101110011011000101011100010110000011111000100000001011101010010011101110010011000110111110010010000111011100111111000000001100100011011001100111110100111111010101010011100000110110100011011101111011001111110110111000110110001110111001111110100110001001110100000010110110010111101001000010101101001100111011001011010110010001110001010110110111100001110111100011001101100110000000100111110011100011100010010110011101001100010011110000000011101010010010011010000001101101101010010011011110001010100111010011100100000001110100011010011100101010011011101110001000111101101101101100110000000110111111110110110010101001000000101001000100010101110111111011011011011010100001000011011111011001100100110010101001010000011011110010101111000011011001110001010010101100110010000100001101111100001001110001000011100111101010110010110010010001100111000010001011000101011100001111110100010111101000001011001000110011001100110111111010011011000001001101101110101001100011110110110011100110111101000010110100010000000101011100100010110100110111001010100011101010111000111101001011101111010101110110001100000011011001000110101001010011101010011101111000001001010101100110111011010011100111100101010000011101111001000101011100110011001000110011100001101111101101100100100000110110011000000111010010111011111011100111010001001011011101110101001000101011011001010101101011011101000001000111011111111100111011100000000011011100110011010001110001010100100101100011000110110011001101100110000000011011100111101000110010110100110110001010101100100001101001101101101100101010101001100010110001101100111101000100101000100101000001001011000110000000101111000100001110110110101011101101010000011011101010001101000011111101001011011000111011101100101010100010011101100011001001001101110110001101000001011001000111001011010010111010100100111010110111110011010101110100011110100011011110011001100010101100110110110010000110110011011010011001111000111010001101010111010001001010010011111101111011001011100000010101000010101000000010000111001110001111100110111001000001001110100110100110011011010111001001011110111011110000000111011100011001110111001000001010110000000101100101010010011101100011111100101001101101100010100000010011101001010001101101111011100100100111001101100001001001011100000011001010010010011101111001101010110010001001011011111110110000100100000010110101001110101010010001001101100110100011101010101100110110111001110100100100011110011001011011101010001101100001011001011101110101000000101000100110101100110011001011101001001010111000010100010011001111001010000100110110110011100011010101111110000000000010111010001110000100101110110010100011100001111010011010101110010110110101100011110101111101100110000101111000000110101100100111101101000000101000001100111000011000101100101101100010011100000101111110001110011110101001000011011101100110001011011011010101010110101001111110110111100110010110110001001101001100111100001010010000011011001110110001111101010111000101100010111100110101011100000101100000000110001110001000000110110110101001001101111010101010011110001000010110010111100110110010111100100101101110010000000100000110111110110011001011100011010010100000010000011010011101000101110110011001111011100000110001010010010000001010000011011110011010010100000010111100100101101101000100001101100111011011101000110111110001010110000000110101101010010011101100001011110111100110100110101011100100011010011010100100111011101111001000111101101010101100110000010010101111110110001000101100100111000100111001110101001011100111001010110001110000001101110010001101100111100111010100011100100010001111000110100011110110111110001101011000011001110001001011110101000001101111100010000010010111101100000111011110010000010111001000000000001100101000010101010111000011011001110100000000100111000111011110100011110110111100100100101011110111011100100111111111000100110111001001001010111001101101000111001111010100111011100001100101100101100110000111100000101101001000111001101010100111100010010111010101110110100010011110110111000111010011111001101010001011010111001111100010100110011110011010100011000100110011110001101000000010101001011011001010110001110111101110000000011011011011110001101100111110001111111001001011101010000001100001011101001101001010001100100101000111010000010100110111001011000101100011100101011001010100101000000010111011011101100001011111011001010001000001111100000110100111100110100101110001101011100110101000101100011010000101000011101101011011000101110111100111100100011110100000001110110001011100100000010001101001011010011011101010000011100100110011010100001110100111000010100011011000010110001000010010010001000101111011101111001110010111100100101110001100011001010000110011101010000110111110010000010110000111100001001011110101000001101111000110000010011010001010010110111000001100101100100001101111000000000101011110011110101110010011111100101100101101010011100011100011110011100100010011000001110110111011011010010010001101100010101100101101111000001011100011000010101011010000110110000101100101100110100010000010111001011000111100111010101110011011110110111001001010101111011100011101100111110110111001011110110110101101001110100000010110111000111010110100100000101111100110000000101001001110011011110011110110010010011000101100110001100000010101110001110110000110110001010100011000101000001010110010001101100011000111010100101101100010010000000111101101110100000010011101111111111001101111000010101100111001001111101110011110101111010011001010011010100000100000111011110110111111100001010110000101001101111001101011011111011111110001101100000110100000110001001111001111010111001100110101000010000001010000000111101101111000010010011101010101111001010111000100101010100010000010101100100001110100110000001100110010110001101010010011000101101110110111001110100000010011100100111110100111111100100000010001101010000001111101101110010100111011101100101101000011011100110110100110100010101101000100001000010101100111000101000011101010111101001001001110010100010000001000010110101100001100001101011011011100111100111101010001000101010110011100100101101110100000001111000110101000111100111001010111101011101110001110100000001010100000100111000111000110101100100011111111011110110100010011010101110110000010100000010010110101010011010000000110110011000110000110110001010001110001110001000111100110110110001100010000110101100000000000011011110001101100111100100011001001110111011011000011101101101110001100001001110010000010010101101101100100100111010000010101100000001110011011111100001001000011100011011110000100100010011011111010100100011100111111001110100110110100000001011110011000111111000001100000010100111011100001000101001110010110101110010111011110000001000100100011111111010110101001000000011100010111010000001010101111011101101010100001001111100001111011011111010000111010010110110100010011001100011101011001100111010111010101011000111101001101000110101101111001001010011110111011001001111001101101100000110101000000010110011010111100100001001100111011010110100000001101111011110101110110100001000111100111100110111000111000100010111011111111110010111001111010011010101000010111011000001101010000000110011111011011100011011101000000101111100010001001011100101110000010011100011101111101110101000101100010110010100000110111011011100000001000111110011110110110011111101101111000000100011100011000100010011101011001111001001001101111000000001000100000000101001011100100011010000110010111001010010110111011010010100100100000011010110101001000110001010101011010000111111001010001011101001101011101011010011000111100101100011101001100110100000101011110001010111001010011000100110111001100000001101111101000110000110110100000000101001001011001011011011000001010101111011100000110011001100001010110111100010100000011101101000000101010010000110100111010100111100111001011010000001110101010110001100000001011110100101101101100111010101101010011110011101000100110000000010001110100010000110111001000011011001110010110001000110001001100100011001111000000101001110001001001011101111110001000001110010000010010100000011010010110100110111010011000000100110011011001011011101101000110001101100111001110001010010111101001101010101100110011001110101100011111010011100101001110011101100001101101001101111111001110000011101100100101100000111001011001101000001110101011101000000100111010101001011001101111110010001101111100100101011001000100101001111111011110000001000101011000100111010011111101001110011111100111101111110011100111011101010000000101110000011110001100011000101101010111011000001011110111000111101111101001011110000101010110111100101001010001110010011010011100110001101000111010100111011101001011010101101011110100110011010000001011101001111011101100011010111000100111100111101000100110000001100001110110110110001010001100010010110111000101000101111011101101100101101010000000001010000001101011010010011011101101100101000111001011100100011011001011001001001100110001011101010001101111100110101001111101100110011100111001000001100111101001101100110100100111111011110010101111110100001101011101000001101001101010000100001000101001000100110111110000010100111110001110001110111011000101110011011101100110000011100000110111111001000000011101011011010010011011100001100100111110011010101101000111100011100100111100001010011010001101111110001011000110101100101010011100101001101101001001111110110101100010101100101101100110111000100001011101001001000110100111111011011110000011111001000110011011100011110101100010001100100111110100111110011101010010011110001101001001111010010001011101010100000110011010110000010010100000010100000110100111100001101000011001111101100111011010010010101011001001100011001011011110100000010001101000000111010110101000001001001100111001101100011000100111100011111001010000100111011100101100101011100011011000111101010001010011001000000001101001111000110110011010100100011001110000100000000010011111000001100000111011101000111100000100000100110111110010101001101010001111110001100110111111100100001011100100011011010001010110111110101101010011000101101010001111110011100011111101100101101011011110100101101011001001110100100111110101010111100010101000111011101111001101010111101100101010000110101000000011000100101010000100111101111011000101001111100110100011000101100101101101010001100000101100111111110001110100101000011011110011001101011001100101101010011011100111010101110100110111000000101000101010011100011001111010100000101000001111111000001011011000111010011100010011110100000000110000001011100001110011000111010011001100000010100110101110111100110000110101010011011111001010101101010001001000111100100100110110011101101001110110110110111101010111111011000001100000000110111010011100100011000110010010001001110011110100011001100000110111001011101100111111111101011001100111100100111100010110001010101101001010011011100110111110011100101110110100011100011100111000000010010000100011101101111010000010011100010110001101101011010100000011011101110100101100111000001111011101000011100011010111001010000110101110100110101011110100110101100001000110110100000000110101001001101011010111101010000010110010110111101100001101001110010001110010110001011011001001101001001011110111001001101010010100010110111100000101101110010001110010101010001110111101110010001101100100011110001000110111011001100011000100001001110110111101010110011100011100100101010011011010111010000011000110001111111010111010010100000111011001100110100001101000101001000100000101010111100011011011001100000000011011001111010011111010110101010001101110101000010111100111110110111101010001101110100101010011011001100100100010001110110110110100000101010100000010101001010001011111000100001101110100010011010110010011110100110010010101100111011110101000000011000100111001010001110110110100010110110011100111011100011011000000010101101011001011011011100101011000011011110111001010001101101110101010100001100110010101110100000010100100001111110100111000010100000001001111000001001010100001001101111001000001111110011010001101110111010011101000011111110100101011010101000000111101101011000110111010001110100111010010011011010010010111011000000001011110011101000101101000010010100001100011101100001000010010000111100100001000000110011111001111011110001111001011100001110101001111100000110000001010111101011111000000001111011000101011001011011110000001011000110010111100010100001010010000001100010111101011011011101100011100010010101000000111110110111110000011101110100011011110010011011010011110010110100100101100000101000111110110011111001110011011110000001110001000110010010110110110010100000101000000101010011111110001100100001110010100101110000010101001010010111110010100100100101100011110011011110100001000000000101000111100000100100111001010110100110101011110100111000000101001100011110010100000100000100000011001101000001111100010011001010011001101110110110000111111101011111000101101001010001000000101001110111011011101100111010101101000000010110010101100101101110110010101010101010000000101101100011000101011011011000110000001110100001101001011001100011010011101001001011011100101010111111011101101011100110000000101111011110110110100000111001011001000111001110100010001100111001000110100000000011101100010001101100111000101110100011000101100000000010100011101110001011011101111011111101011010110100100011011011010000010100111100100000010110100101100100110000101010100000011010010100011110110000100100001011011100001100101100100000000100111101000001111110000001101110010001101100111101111010100010000000101111110001010001010000110111101001001000011100001001001001101111100110101011101000110110000110111011001100000110001100111100110101101111110010010101011110100011100010110000110111000101001100011110100011010001101010110101001001110011100101000111001000000111000111110001011011011110000111000110000101100110101111101001100101110111101100110011011011101001101111011100101110010010001000001101100010110010101110110111111011111011000101100011001101001010110000100111100001010110000010010100001000000110100001000010111010110011000000001011110010011110010110010010000011000100110101111100000001010010101100110110001100010100010000111011000001010011110101010011101011101110101011110101101010101011100100011010011110101100111011110011001100111101101110101100110000100010101111110110001100011110100111000100111001110110001101100111001011100000110000001110110010001101101011100111010100011101001000110010110011101101101011100100001101101111001001110111110011100010110100110111011000111010011101000011110100111001011001101000000100111000010101111101111110000001100110011000000011101111010001100000101110010110000110111110000010100111101100111001110110111100111110010100101100100010101100011111101111110000100001101100100011101001111101000111001111110011100101010001110011101111000001010111101011100000001010011000100101001100100111110001101010000001010100101010011111101010101011011110010101000111011011110011001010010110100000101100010011110010001011011001011110010010111111100111100001001011011110100101101000101010010010101001101101011101000001100011000111111101011101001010000011101100110011010000110100010100100010011010100011110011101101011001100111111001100100101010101100000011010110001010010111000110011000001110111000011000101110101000000101110000011111011100110010000000001011110111011110011101001110101001010110001100001100011110011011100111101010110001011010001100011110000101010100100001100011011101001001110111001000011001110011001010111010001001011001011100010100111111110010101110111000110110010110011111110110110000010000010000010110101100000110011110010110111001101010100011110101000010001101100111001101111000110101100111111000101111011101111001110010111101100101110000000011101000000101100000011000110111110111100001110101101110001001011110101000001101111100010000010010111101101001001100111001010100110110011000010010010011001001100011101100000011011001000111101011011011100111000001101011001111010000000100110110011111001001010100111110010000010101011001001110111011001001010000101101110000110101101000000110110011110110010101010011001011000111000111001010110000111110110011110011100100110011010001100110011110001100100011100100100110111011000011011101100110101010000000101010110010010110000010101111001001100010100000111111001100111101111000010101011011011101011001101000101000111110001110011011111110001100101001111000001100011001001110000111000100101110011000000001101110101100001000110001111001101011110101011000011011110100000001011001110100110101111011010111000001101000110100101101100111001000100000001000001011010001010000011011111111001011101010101010111000100100000111110110111100101111001110111100000010110011101010111001100011000100010101111001001101001010011111000001100111010011000000011110011011101110000000010101110110001010101110011001100111100101100000011011000100011000101111000010111010110101110110101001101111001000000011010100100101100011000001011100010011100001101100111010000000100011100010100100101010111100100011000001101001100101111000110011101010111000110111110110011001010110000100010100000010111101001011100000101010110010000111111100000001001110000101011110000000101100100001010100111111111110000100000001010110001010000010110100101001011101100001100011011001111001101110110110110001101010000001101100100011011100111011110000001001100000001010001110001101000101010001101101100100010101000110110101000101011011110011001110110101011010010100010001000000111111010100010100001000010000111100110011110101001010111011011100001010000110110010111011101010010111100101001011010101010000011011101101010100011011011111001111110011000110111001110010100010100110001001010110011000000001001001101111001110010001110100010010111010110101000100110011101100000100111101011101001111011001010101110000111000110010010110010000010111001111001110001100110110101000010000000100100110111001110110110100101101101100000101000101100101010010001110110000011011111001110101001110001001010011101110001100001011111100111011000100110011100010100110100110011101010101000111010011001100000000101110011110100000100101001100111010100001001110101110010010011011110010111101010110101010001101111001111010001101101011111100110101001011100011011001010000110110101110011001001010100100100010011001100000110010001100001111011101110100101110010011110110111101100101001111101100110101010000101000000001011100110110010011011111101101000011101001011101000111001010000100000001110010101011101011000000110100110010011100000101000010101001001010110101011011010001001101010111101001001010000000101100000100001101000000011011001100011000011011000101011001000011000110011110011011010011010001000010010010001001111101100111100000110110001010110111011111000111001111010100000001101100101011011101110110111001110010101011101010010010001010101011111110101100001010110001111110111011110011010101011001100110100010111100001001010011101010011101100010010010011101100110101001110100000101000001101011000100111000110010000000010101110111000011001011001011001101101111000000001010010011110011010101010000001111101000101110010100101110110110011101011011110100001101111000001000110000000011000111010011001100110110110111101011001100101010010100010011101111001110100101010101111001000101010100010111001011110110011000001001010110011001010101010110110000011010100000111110111011100000011100101001011001000001110111100000010100110100010010001010100001001000010100000001101001110010100011111000110000000100101001110001001110011101100010100001110010111110101110000011101100110001010111011010101100000011010110001110010111001000001110000001100110111011111111000110001000111011001110010100000111110001001010011110111110101000001101111001010011011101101101000011001100000011000001111110011100101100011001100100111000001110011110001000101100100010010100111111111110000010111001010111100001101100111000101101010110011111011111001101010100011100110111001111001001100100101101010010010001000000101001111000110111101101110111100010000000110110001011011100011011011110101001001111011011111100111111000000010011101011100000100011001110001101001000011000000100111100100011110110000100011010110110000100011100011000110111110110000000010110110000000010100100101100101101101100000101010111101110000011001100110000101011011110001010000001110110100000010101001000011010011101010011110011100101101000000111010101011000110000000101111010010110110110011101010110101001111001110100010011000000001000111010001000011011100100001101100111001011000100011000100110000010110000011001010011011001011000111000110011000010100000010000111110110010110000110001001001110100100010001100110010011011100111010110100001111001100111110010110011110001110010011110110001100111000010001100001011001010010111001110010100100100111011001000000000000000100111100011011010000001010110010110101000011000010101111011100110010111010100100101000011010000100001110110000001101100110000100010101110100011011110111100100111100001001011011101001000111000111011001111011011001000011110110111010000001100011101100010100100111101010100101101111000101111100010101101110100101111001010100011011110110110001000110101010101000111011010100011000001010011111000101111011011111110100110100001101101100100111000001011000101010010110110111001110110010010011010101101010100010100000000011010000100011001000000110110010100110000110110001011111010000110001100111100110110110110000010000101010010101110111000111011011001101000101101111101010100111010111010101111010000010011110101001111010101110111111010010011101011011110000001001011001111010001011011101010110101001110000101001011000001000101110101001110010110010001110011101000100011001110010001101000100000111011000100011011001110100010101000110010000100101011101110010100001101110111000001101100101011111100101100010001011101000010111001001101001011010110000011101100111100001001011011110000011001000100000011110101000000110110010101000110100110001100011110010110011101111001101010100001000000000000101001111101101101111001101110100110101011010100111000000101000100111100010100100100000100000101010001000001100110101111011010010010001010110111000011001101010000101001011101010011001101110011010101111011001000000001111110101101010110001100000000110000011110100110111011010001010011010100010001010111001110101110000100111000010110111001101010110101110011101111010110101011100011100010111001111001010000110111110111110010001010111000001010110001100101010011011101101101001001001111001010100110100101110011011101101110000001011111010000110010000100111101011001100111011100101111011011010001010100110101110010101011001100011000001111110101011101010011100010111010011010110110100101100010111101000001110101001000000110100001001001000110011000100001101111101100011001110100001000110011100111111100100011111001110001010001001110100011101010100110111110101011101111100000001101110000110101101101111010011111001000010101001010100010101010010110001101010110011101111100001010100111011011100101110101110000100001011001000000110000010101000110100001101100010001011100010101110000110110011100101111000100111010100010111001011000010001001111110100110001110111100001011001101000011011010110100010111001010011000001101110101010001001111011101111000000011101110101100111011010011101110100101010001110011010100100010011010101110101010011101101011010010010010001001000101101110110100011110101001100100010011101111001011000000111000010100101011010011101110111000101000110110000000000111101000100010111101001101111010100110001110101011010101011001101000100011011110101000001101101101001100010011100111111000111100001011100011110110110101001101110001101100011011100101011010010100101101100001011101111010111001000010010010100110001111111001111101111110100100111101001001001101110111100001101010111010001011000100101010111011100101111110101111011110011010110100101100100010010010001001111011011110000111001110001010111111000000100101011101110001101111011101011010010100100100010100011110111011000111100101101000101000101010011101010011101100110000110011100100001100011001100110100101110100000100011111001110111111101010110100111110000011000001100110110101111000111010011101001010111011110000010101111000101011100010011101110111110101000101100110000101000111100001010100000100011010011111001101010011111000110111101000110110000100101100011010000101000011101110111011000111101111010101001010011110000100111100011011101100111010010000111101101011001101101111001001100001110010101110110111011110100001111011011111001011011111000011110011110010110000111011011001011101110010110011010010011100110101001110101001110110111000111001110011110111101100110110011011000001001111100011010110110101110101011001100101110111100101010011000100000011011111001000001101100011101001101101110010000101100100101000100010111000000101010000100111101110100000101101110100111110011011010111000011010010011101101011110100100010100100010001101010011110010001011011011111100110100000001101110010110001101100010111000101101000001111011110110100010011010101110110000010100000010010110101010011010000000110110011000110000110110001010010110000110001010111100110101010110010010010101101111011101011010101010100100001100010011100101011011110111011000101011110010010100010001010000010010000110001101011000100101011101110010111111011011110011001011011010010110010001001001000100111101101111000011100111000101011111100000010010101110111000110111101110101101000110111110100011010011101010001000000000110101111001000100011111001111100111110001101111111010011000010101100111110011101110100101100010010000111001000110010000111110110111010010001111011101101011100101000110110001001101100110111010011111001001100101010110011001111000000110110011000010010011110010001010001100111010001001011011100011001010001110111100110010110111100111010011001011011001110110101000001001111001110001101010111100001010100101010010001110111100110110011110110010001001111000000110010101111101011100000001010110011011000110110010111011010110101110001011111000000010100001010100000001100100110010110010011001010110100000010101000001110110001001000000111111101111010110111110000000110111010101001110110000010110000100110011000111010110011000110101110101010110000111011100100011100011011101010000000001110100100100000011101100100101011001001110110000011010101110001101000110001010010011110100111100011111011100011101011100000011010011111101101110101100001000101000111000011000100100000001101100110011101000010000010101010100111000010111100101100010100000011001001100111110101111101110111100110010110111000011000110110110000011111100011101000110110011111010100010101010100111000010101101010011111100110101001111011011011010001011001001111011011010101110100000110000011011101010000100001110010110101100100110111011001101001111110101000000101101010011111101101111011000101101100001101010010110000110011011010111010000010010110000111111100000000000110000001100111100000101101000000100110100100110000101111011001110100000010100000001000110011010110010111010011110101011100110110110011110010110010010111010110110011011011000011010110000100001001100110000100100100010001110101011010000000001111101111010100001011110010001110001100101010101001111001001101101110001000110110110010100101110010010011000011100100001001001101111101100101011000100011111000000011101000111001000110101001101000001101111010100110000100011100011001101001101011001011011000000011000101100001011010101001110110000000101110011101001000100011011110101010010010011001001110110001001001001011110111110001111101101000011101010101111010011101001010100101101010101001111000011100011011110010111101010100101011001001010111110100110111110010010110110011001101010111001001100010010001101110000111011111110010101001110001111010110101011011110010100101000101000001101010110010000101011000000101100101011101101110110011011100100111111011001111001110011101111010001110001111011000100101001110101010001000010001101001111110101111000111100011011100111001010000011001110100101010010111111101000011000000010111001110110001110111110000000010011101010111101001110010100000110111011100110011110000001000010110011110101010001100000101110110101000111100010101010111010000000010101100010001011110100100000110101101000000110010101001100010011101110101010010101100111100001001011011111010011001000111110011000100110111100111010101000110000111000110000011010001011001111100010110000011001100101011110000000110111001010101110111010100001100111110100001101111010001010110101101010011101100111010100011010011110001010000000101000010011100001001011101010101111011001011101100100101011100110011011010100100011010111000101001100010101011000010011101101111100100011011100111010100111000110101001010010100001001100101001101100010110100101000011000000100111000011000110110110010001111101000000101100001001110110011111100001001111011010101100101001011000101010011101001100111001100110111111011000100111001110010111010001101001001000111111011010010110011101111000000011101110100011101010110111100000100111100010001110110101100001100011010110100011010001011100011011001011101010010110111001000001000000110111010010010001010000000001011101100001110'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_HammingCRC16(text):\n",
    "    #переход к бинарному представлению текста\n",
    "    txt_toBinary = (bin_code[bin_code.char == x]['binary_code'].values[0] for x in list(text))\n",
    "    binStr = ''.join(list(txt_toBinary))\n",
    "\n",
    "    #разделение на 30-битные информационные слова\n",
    "    bin_package30 = [binStr[i:i+lenw] for i in range(0, len(binStr), lenw)]\n",
    "\n",
    "    #дополнение нулями последнего слова\n",
    "    if len(bin_package30[-1]) != lenw: bin_package30[-1] += (lenw - len(bin_package30[-1]))*'0'\n",
    "    #добавление контрольных бит и контрольных сумм CRC16\n",
    "    DataBits = []\n",
    "    for i in bin_package30:\n",
    "        tmp = list(i)\n",
    "        crc = divide(str(int(i + 16*'0')))\n",
    "        if len(crc) != 16:\n",
    "            crc = (16 - len(crc))*'0' + crc\n",
    "        for j in positions:\n",
    "            tmp.insert(j, '0')\n",
    "        for j in positions:\n",
    "            sm = 0\n",
    "            for k in range(j, len(tmp), 2*(j+1)):\n",
    "                sm += sum(int(h) for h in tmp[k:k+j+1])\n",
    "            tmp[j] = str(sm%2)\n",
    "        DataBits.append(''.join(tmp) + crc)\n",
    "        \n",
    "    return ''.join(DataBits)\n",
    "\n",
    "encode_HammingCRC16(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При добавлении ошибок случайно выбираются 100 информационных слов. Функция добавления ошибок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_errors(binStr, n_errors):\n",
    "    #разделение на 52-битные отрезки (30 - информационное слово, 6 - контрольные биты, 16 - контрольная сумма CRC)\n",
    "    n = 52\n",
    "    bin_package52 = [binStr[i:i+n] for i in range(0, len(binStr), n)]\n",
    "    #добавление ошибок\n",
    "    ErrorsBits = []\n",
    "    chosen_words = []\n",
    "\n",
    "    all_words = list(range(len(bin_package52)))\n",
    "\n",
    "    for i in range(100):\n",
    "        chosen_words.append(all_words.pop(all_words.index(random.choice(all_words))))\n",
    "\n",
    "    for i in range(len(bin_package52)):\n",
    "        if i in chosen_words:\n",
    "            tmp = list(bin_package52[i])\n",
    "            all_chars = [2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14]\n",
    "            all_chars.extend(list(range(16, 31)))\n",
    "            all_chars.extend(list(range(32, 36)))\n",
    "            \n",
    "            chosen_chars = []\n",
    "            for j in range(n_errors):\n",
    "                chosen_chars.append(all_chars.pop(all_chars.index(random.choice(all_chars))))\n",
    "            \n",
    "            for j in chosen_chars:\n",
    "                tmp[j] = str(int(tmp[j])^1)\n",
    "            ErrorsBits.append(''.join(tmp))\n",
    "        else: ErrorsBits.append(bin_package52[i])\n",
    "    return ''.join(ErrorsBits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция декодирования с исправлением ошибок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_HammingCRC(binStr):\n",
    "    #разделение на 52-битные отрезки\n",
    "    n = 52\n",
    "    bin_package52 = [binStr[i:i+n] for i in range(0, len(binStr), n)]\n",
    "    #разделение на информационное слово с контрольными битами и CRC\n",
    "    for i in range(len(bin_package52)):\n",
    "        bin_package52[i] = [bin_package52[i][:36], bin_package52[i][36:]]\n",
    "    \n",
    "    DataBits30 = []\n",
    "\n",
    "    for i in bin_package52:\n",
    "        tmp = list(i[0])\n",
    "        #извлечение контрольных бит\n",
    "        for j in [31, 15, 7, 3, 1, 0]:\n",
    "            tmp.pop(j)\n",
    "        DataBits30.append(''.join(tmp))\n",
    "\n",
    "    DataBits = []\n",
    "\n",
    "    wrongWords_found = 0\n",
    "    fixed = 0\n",
    "    crc_wr = 0\n",
    "    er_fixed = 0\n",
    "    for i in range(len(DataBits30)):\n",
    "        #подсчет контрольных бит и CRC\n",
    "        tmp = list(DataBits30[i])\n",
    "\n",
    "        crc = divide(DataBits30[i] + 16*'0')\n",
    "        if len(crc) != 16:\n",
    "            crc = (16 - len(crc))*'0' + crc\n",
    "\n",
    "        if crc != bin_package52[i][1]:\n",
    "            crc_wr += 1\n",
    "\n",
    "        for j in positions:\n",
    "            tmp.insert(j, '0')\n",
    "        for j in positions:\n",
    "            sm = 0\n",
    "            for k in range(j, len(tmp), 2*(j+1)):\n",
    "                sm += sum(int(h) for h in tmp[k:k+j+1])\n",
    "            tmp[j] = str(sm%2)\n",
    "        #сравнение с исходными контрольными битами и crc\n",
    "        sm = 0\n",
    "        for j in positions:\n",
    "            if bin_package52[i][0][j] != tmp[j]:\n",
    "                sm += j + 1\n",
    "            #извлечение контрольных бит\n",
    "        \n",
    "        if sm != 0:\n",
    "\n",
    "            wrongWords_found += 1\n",
    "            #исправление ошибки\n",
    "            if len(tmp) >= sm:\n",
    "                er_fixed += 1\n",
    "                tmp[sm-1] = str(int(tmp[sm-1])^1)\n",
    "\n",
    "            for j in [31, 15, 7, 3, 1, 0]:\n",
    "                tmp.pop(j)\n",
    "\n",
    "            #повторный подсчет crc\n",
    "            crc = divide(''.join(tmp) + 16*'0')\n",
    "            if len(crc) != 16:\n",
    "                crc = (16 - len(crc))*'0' + crc\n",
    "\n",
    "            if crc == bin_package52[i][1]:\n",
    "                fixed += 1\n",
    "                for s in tmp:\n",
    "                    DataBits.append([s, 0])\n",
    "            else:\n",
    "                for s in tmp:\n",
    "                    DataBits.append([s, 1])\n",
    "        else:\n",
    "            for j in [31, 15, 7, 3, 1, 0]:\n",
    "                tmp.pop(j)\n",
    "                \n",
    "            if crc != bin_package52[i][1]:\n",
    "                wrongWords_found += 1\n",
    "                for s in tmp:\n",
    "                    DataBits.append([s, 1])\n",
    "            else:\n",
    "                for s in tmp:\n",
    "                    DataBits.append([s, 0])\n",
    "    i = 0\n",
    "    fin = []\n",
    "\n",
    "    while len(DataBits) > 7:\n",
    "        sm = 0\n",
    "        tmp = []\n",
    "\n",
    "        for i in range(7):\n",
    "            sm += DataBits[i][1]\n",
    "            tmp.append(DataBits[i][0])\n",
    "        if sm == 0: fin.append(''.join(tmp))\n",
    "        DataBits = DataBits[7:]\n",
    "    for i in range(len(fin)-1, len(fin)-5, -1):\n",
    "        if fin[i] == \"0\"*7: fin.pop(i)\n",
    "    BinarytoText = [bin_code[bin_code.binary_code == x]['char'].values[0] for x in fin]\n",
    "\n",
    "    return ''.join(BinarytoText), wrongWords_found, fixed, len(bin_package52), crc_wr, er_fixed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отправка без ошибок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модели обработки естественного языка (Natural language processing, NLP) на основе архитектуры Трансформеров, такие как BERT, RoBERTa, T5 или GPT3, успешно применяются в самых различных задачах и являются стандартом современных исследований в области NLP. Гибкость (универсальность) и надёжность Трансформеров способствовали их широкому распространению, что, в свою очередь, позволило легко адаптировать подобные модели для разнообразных задач обработки текстовых последовательностей, как в качестве seq2seq моделей для перевода, суммаризации, генерации текста и т.д., так и как самостоятельного энкодера для анализа тональности, частеречной разметки, машинного чтения и др. Главным изобретением Трансформеров стал механизм внутреннего внимания, который подсчитывает метрику схожести для всех возможных пар токенов входной последовательности независимо (параллельно), что позволяет избежать последовательной зависимости рекуррентных нейронных сетей. Благодаря этому механизму Трансформеры существенно превосходят более ранние модели обработки текстовых последовательностей, такие как LSTM.\n",
      "\n",
      "Однако модели на основе Трансформеров и их производные имеют свои ограничения. Так, механизм внутреннего внимания требует вычислений, квадратичных длине входной последовательности. С нынешними мощностями и размерами моделей это обычно ограничивает входную последовательность до 512 токенов и не позволяет напрямую применять Трансформеры к задачам, требующим большего контекста, таким как вопросно-ответные системы, суммаризация документов или классификация фрагментов генома. Естественным образом возникает два вопроса:\n",
      "\n",
      "1.\tМожно ли достичь тех же эмпирических результатов, что и традиционные «квадратичные» Трансформеры, но используя разреженные модели, у которых требования к вычислительной мощности и памяти возрастают линейно с увеличением входной последовательности?\n",
      "2.\tМожно ли теоретически обосновать то, что эти «линейные» Трансформеры сохраняют выразительность и гибкость своих «квадратичных» собратов?\n",
      "\n",
      "На оба эти вопроса авторы постарались ответить в своих недавних статьях. Так, в «ETC: Encoding Long and Structured Inputs in Transformers», представленной на конференции EMNLP 2020, было предложено построение расширенного Трансформера (Extended Transformer Construction, ETC) – новый метод разреженного внимания, в котором используется информация о структуре данных для ограничения числа подсчитываемых пар оценок сходства (similarity score). Это позволяет привести квадратичную зависимость к линейной и получить хорошие эмпирические результаты в NLP задачах.\n",
      "\n",
      "Количество неверно переданных информационных слов: 0\n",
      "Количество исправленных ошибок: 0\n",
      "Количество инф.слов всего: 598\n",
      "Количество инф.слов с неверным crc: 0\n",
      "Количество исправленных битов: 0\n"
     ]
    }
   ],
   "source": [
    "txt, wr, fixed, nw, crc_wr, er_fixed = decode_HammingCRC(encode_HammingCRC16(text)) \n",
    "print(txt)\n",
    "print()\n",
    "print(\"Количество неверно переданных информационных слов:\", wr)\n",
    "print(\"Количество исправленных ошибок:\", fixed)\n",
    "print(\"Количество инф.слов всего:\", nw)\n",
    "print(\"Количество инф.слов с неверным crc:\", crc_wr)\n",
    "print(\"Количество исправленных битов:\", er_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модели обработки естественного языка (Natural language processing, NLP) на основе архитектуры Трансформеров, такие как BERT, RoBERTa, T5 или GPT3, успешно применяются в самых различных задачах и являются стандартом современных исследований в области NLP. Гибкость (универсальность) и надёжность Трансформеров способствовали их широкому распространению, что, в свою очередь, позволило легко адаптировать подобные модели для разнообразных задач обработки текстовых последовательностей, как в качестве seq2seq моделей для перевода, суммаризации, генерации текста и т.д., так и как самостоятельного энкодера для анализа тональности, частеречной разметки, машинного чтения и др. Главным изобретением Трансформеров стал механизм внутреннего внимания, который подсчитывает метрику схожести для всех возможных пар токенов входной последовательности независимо (параллельно), что позволяет избежать последовательной зависимости рекуррентных нейронных сетей. Благодаря этому механизму Трансформеры существенно превосходят более ранние модели обработки текстовых последовательностей, такие как LSTM.\n",
      "\n",
      "Однако модели на основе Трансформеров и их производные имеют свои ограничения. Так, механизм внутреннего внимания требует вычислений, квадратичных длине входной последовательности. С нынешними мощностями и размерами моделей это обычно ограничивает входную последовательность до 512 токенов и не позволяет напрямую применять Трансформеры к задачам, требующим большего контекста, таким как вопросно-ответные системы, суммаризация документов или классификация фрагментов генома. Естественным образом возникает два вопроса:\n",
      "\n",
      "1.\tМожно ли достичь тех же эмпирических результатов, что и традиционные «квадратичные» Трансформеры, но используя разреженные модели, у которых требования к вычислительной мощности и памяти возрастают линейно с увеличением входной последовательности?\n",
      "2.\tМожно ли теоретически обосновать то, что эти «линейные» Трансформеры сохраняют выразительность и гибкость своих «квадратичных» собратов?\n",
      "\n",
      "На оба эти вопроса авторы постарались ответить в своих недавних статьях. Так, в «ETC: Encoding Long and Structured Inputs in Transformers», представленной на конференции EMNLP 2020, было предложено построение расширенного Трансформера (Extended Transformer Construction, ETC) – новый метод разреженного внимания, в котором используется информация о структуре данных для ограничения числа подсчитываемых пар оценок сходства (similarity score). Это позволяет привести квадратичную зависимость к линейной и получить хорошие эмпирические результаты в NLP задачах.\n",
      "\n",
      "Количество неверно переданных информационных слов: 100\n",
      "Количество исправленных ошибок: 100\n",
      "Количество инф.слов всего: 598\n",
      "Количество инф.слов с неверным crc: 100\n",
      "Количество исправленных битов: 100\n"
     ]
    }
   ],
   "source": [
    "txt, wr, fixed, nw, crc_wr, er_fixed = decode_HammingCRC(include_errors(encode_HammingCRC16(text), 1)) \n",
    "print(txt)\n",
    "print()\n",
    "print(\"Количество неверно переданных информационных слов:\", wr)\n",
    "print(\"Количество исправленных ошибок:\", fixed)\n",
    "print(\"Количество инф.слов всего:\", nw)\n",
    "print(\"Количество инф.слов с неверным crc:\", crc_wr)\n",
    "print(\"Количество исправленных битов:\", er_fixed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Символы, содержащие биты из неверно переданного информационного слова, пропускаются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модели обрабстественного языка (Natural language proce, Nа основетектуры Трансформеров, такие как BERT, RoBERTa, T5 или  успешно применя самых разлизадачахяютандартомеменных исследований в области NLP. Гибкость (унивеность) и надёжность Трансформеров способствовали их широкому распро что, в оче позволигко адапподобные модели для разнообраадач обработки текстовых последователей, как в качестве seq2seq моддлявода, суммари, генерации текста и т.д., так и как самостоятельного энкодера для анализа тональносастеречнзметки, ного чтения и др. Главным изобретением Трансформеров стал механизм внутреннего вня, который подсчет метрику сти для всех возможных пар токенов входной послеельности независимо (парано), чтооляет избежать послеельной завис рекуррентных нейронных сетей. Благодаря этому механизму Трансформеры существенно превосходят более ранние модели ткитовых повательностейие кTM.\n",
      "\n",
      "Однако модели на оснрансформеров производные имеои ограничения. Так, механизм внутго внимания ет вычислений, квадратичных длине входной последовательности. С ныне мощност размерами моделей эычноничивает входнуюедователь до 512 токи не позт напрямименять Трансформеры к за, трим большего контекста, таким как тветные системы, суммаридокументов или классификация фрагментов генома. Естественныазом возникает два в:\n",
      "\n",
      "1.\tМожно ь тех же эмпирическихльтатов, что и традиционные «квадные» Трансформеры, нользреженные модели, у которых трниячислительнойсти и памяти возрастают линейно с увеличением ой последовательности?\n",
      "2.\tМожно ли теоретическсновать то эти «линейные» Трансфо сохраняют выразительность и гибковоих «квадратичных» собратов?\n",
      "\n",
      "На оба эти вопрвторы полись ответить в своих нед ста Та«ETC: Enng and Structured Inputs ansformers», представленной наеренции 2020, было предложестроение расного Трамера (Extendansformer ConstructioC) –тод разреженного внимания, в котором изуется информацитрукданных для ограня чиодсчитываемых пар оценок сходстваilarity score). Это позволяет привести квадратичную зависимость ейной и получить хорошие эмпиезультаты в NLP задачах.\n",
      "\n",
      "Количество неверно переданных информационных слов: 100\n",
      "Количество исправленных ошибок: 0\n",
      "Количество инф.слов всего: 598\n",
      "Количество инф.слов с неверным crc: 100\n",
      "Количество исправленных битов: 56\n"
     ]
    }
   ],
   "source": [
    "txt, wr, fixed, nw, crc_wr, er_fixed = decode_HammingCRC(include_errors(encode_HammingCRC16(text), 12)) \n",
    "print(txt)\n",
    "print()\n",
    "print(\"Количество неверно переданных информационных слов:\", wr)\n",
    "print(\"Количество исправленных ошибок:\", fixed)\n",
    "print(\"Количество инф.слов всего:\", nw)\n",
    "print(\"Количество инф.слов с неверным crc:\", crc_wr)\n",
    "print(\"Количество исправленных битов:\", er_fixed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
